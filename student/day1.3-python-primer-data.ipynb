{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Primer: working with data\n",
    "\n",
    "## <div style=\"color: #db366d\"> Day 1.3 </div>\n",
    "\n",
    "One of the best things about python is the ability to easily load, explore and manipulate data. As they say, *data is king*, and you can't have good ML outcomes without good data, just like in culinary.\n",
    "\n",
    "![](../images/produce.jpeg)\n",
    "\n",
    "## Loading data\n",
    "Let's start by reading a text-based file without any fancy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a file from the system into the mem space of this program\n",
    "file = open(\"../data/gowork.csv\", \"r\")\n",
    "\n",
    "# extract the file contents into a variable for us to work with\n",
    "file_contents = file.read()\n",
    "\n",
    "# display the contents on the screen\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's really hard to work with a chunk of text like that...\n",
    "\n",
    "Enter the `pandas` library, probably the most important Python library for data science, e.g., see the pandas' prowess below...\n",
    "\n",
    "![](../images/rise-in-pandas.png)\n",
    "<br/>\n",
    "Let's try loading the same csv file with pandas below:\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import both numpy and pandas, which are often used together\n",
    "# - pandas is built on top of NumPy, a lower level library\n",
    "# - NumPy helps you work with large n-d arrays/matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing common dataset formats is ultra simple in Python\n",
    "df = pd.read_csv('../data/gowork.csv')\n",
    "\n",
    "# the first thing after you load data is always to peek at it\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can also display using the Jupyter Notebook's theme\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic things you can do is to convert a dictionary to a dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "handmade_dict = {\n",
    "    'Breakdown' : ['Slow', 'Yes', 'Slow'],\n",
    "    'Rain'      : [True, True, False],\n",
    "    'Oversleep' : ['Slight', 'Major', 'Major'],\n",
    "    'ML Lecture': [True, True, True]\n",
    "}\n",
    "\n",
    "# note we do not have a variable to hold the df below\n",
    "pd.DataFrame(handmade_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load a lot of common data types in here with Python and pandas.\n",
    "\n",
    "Try the following to load a json file downloaded from data.gov.sg ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python has a built-in json library to process json\n",
    "# additionally, you need some pandas json libraries \n",
    "# to load it as a pandas DataFrame\n",
    "import json\n",
    "from pandas.io.json import json_normalize # load a specific function\n",
    "\n",
    "# load the file into data var\n",
    "file = open('../data/datagovsg/weather.json')\n",
    "data =json.load(file)\n",
    "\n",
    "# convert raw json data into a DataFrame\n",
    "df = json_normalize(data['items'],\n",
    "                    record_path=['forecasts'],\n",
    "                    meta=['update_timestamp',\n",
    "                          ['valid_period','start'],\n",
    "                          ['valid_period','end']\n",
    "                         ]\n",
    "                   )\n",
    "\n",
    "# peek at the top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or peek only bottom 5 rows\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from the web\n",
    "With pandas, you can load data from the web easily as well.\n",
    "\n",
    "Try the following Star Wars web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the requests library to make *web requests*\n",
    "# we will import json and pandas as usual\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# make a web request with the web API\n",
    "# the response from the API will be store in the response var\n",
    "response = requests.get('https://swapi.co/api/people')\n",
    "\n",
    "# get the json formatted text from the response\n",
    "# store it as a dictionary (dict) data structure\n",
    "# dicts are common programming data structures for key-value pairs \n",
    "data_dict = response.json()\n",
    "\n",
    "# let's peek at the dictionary first, to see what we want\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now convert the required (sub)dict into a pandas DataFrame\n",
    "df = pd.DataFrame(data_dict['results'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universe of Python libraries is vast, and the general advice is to Google for an existing _approach to get your dataset_ (or anything you want to do in Python for that matter) first, before rolling your own.\n",
    "\n",
    "For example, try the follow ultra simple way to get stocks data from the web using pandas_datareader lib.\n",
    "(Note you may need to install it first by issuing `conda install pandas-datareader` command in the terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_datareader consolidates various popular online financial data providers\n",
    "# most are free, or have have free tiers \n",
    "# some don't even require membership, like Stooq, demo'd here\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# fetch Google's stock OHLC data from Stooq and display it\n",
    "df = web.DataReader('GOOG', 'stooq')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: \n",
    "#### (1) Fetch current EURUSD, EURGBP, AUDUSD, USDJPY, USDCAD and NZDUSD prices from freeforexapi.com, and \n",
    "#### (2) display in a DataFrame (each currency pair should be a row of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write Python code to fetch the required data as instructed above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing the data\n",
    "We saw a bit about how to display the loaded data. How about other information that you'd commonly need?\n",
    "\n",
    "Try the following to obtain basic dimensionality information about datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# basic sizes of the dataframe\n",
    "print('SHAPE of df\\n---------------------')\n",
    "print(df.shape)\n",
    "\n",
    "# detailed info about the dataframe\n",
    "print('\\nDETAILED INFO of df\\n---------------------')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get statistical descriptions of the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data\n",
    "Now that we have the data, we will normally want to do stuff to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g., we can extract a single column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Close'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then work with these columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sum of volume is', df['Volume'].sum())\n",
    "print('Mean of open price is', df['Open'].mean())\n",
    "print('Median of all high prices is', df['High'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing is a key concept when working with DataFrames. It basically means \"cutting\" out a piece from the entire dataset.\n",
    "\n",
    "E.g., we can get a slice at row number (i.e., index=) 100..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice that contains the date \"2018-12-10\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2018-12-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing the 5th to 10th rows...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing rows between Feb and Mar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# note that the rows are sorted in reverse chronological order\n",
    "df.loc['2019-01-31':'2019-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing rows from Oct onwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc['2018-10-01':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or slice out records that match certain conditions, e.g., a range of values or equal a certain number...\n",
    "\n",
    "E.g., here's an alternative way to get the previous slice with date \"2018-12-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index=='2018-12-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find row with close price between 1000 and 1050\n",
    "print('A slice that contains all rows with close price between 800 and 900')\n",
    "df[(df['Close']>=800) & (df['Close']<=900)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for display purposes, we'll just leave the sliced cakes \n",
    "on the table as above, however, in most practical cases, we'll assign the sliced portion to a variable, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_slice = df.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data cleansing\n",
    "\n",
    "Some other common things we may want to do with the data includes re-structuring and removing stuff.\n",
    "\n",
    "For this section let's load in the Star Wars dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get('https://swapi.co/api/people')\n",
    "data_dict = response.json()\n",
    "df = pd.DataFrame(data_dict['results'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of ML is knowledge representation, and part of that involves determining what to throw away.\n",
    "\n",
    "After scrutinizing the dataset structure above, the first thing we may want to do is to remove unwanted cols..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that the arg axis=1 tells it to drop the columns (axis=0 refers to rows)\n",
    "df_new = df.drop(['created','films','homeworld','species','starships','url','vehicles'], axis=1) \n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many DataFrame manipulation operations return a new copy of the DataFrame. As shown above, the original DataFrame is untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, unless you specify it to be modified **in place**, like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['created','films','homeworld','species','starships','url','vehicles'], axis=1, inplace=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may want to **rename** some cols to our liking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['birth_yr', 'updated', 'eye_col', 'sex', 'hair_col', 'height', 'weight', 'name', 'skin_col']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're at it, might as well **re-arrange** the cols..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that we're using self-assignment here\n",
    "df = df[['name', 'height', 'weight', 'sex', 'birth_yr', 'eye_col', 'hair_col', 'skin_col', 'updated']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the data, it seems that there are some empty fields (e.g., hair_col), which is common especially in very large datasets.\n",
    "\n",
    "In consequence, we may want to **replace** the non-standard empty fields ('n/a') into something more standard in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN or 'Not a Number' is a standard way to represent nulls in pandas and numpy\n",
    "import numpy as np\n",
    "df.replace(['n/a', 'unknown'], np.nan, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With standard null representations, we can easily do things like viewing **where the nulls are**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or checking **how many nulls in each col**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then after evaluating the nulls, perhaps we may need to **remove the records with nulls**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, the original df is unmodified\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to **drop the columns with nulls** instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is to remove duplicates. We don't have dupes in this data but we can artificially create some like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duped_df = df.append(df)\n",
    "duped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply call the following to **eliminate all dupes**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duped_df.drop_duplicates(inplace=True)\n",
    "duped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of data cleansing is to run some arbitrary function that formats the data to a style that is easier (and often faster) for the ML algorithm to act on.\n",
    "\n",
    "E.g., perhaps we want to **run a function on each row** that converts all dates to our preferred simplified format.\n",
    "\n",
    "First we define the function to run on each date. This function will omit the time portion of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "    return date[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply apply this function on every date in each row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['updated'] = df[\"updated\"].apply(format_date)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: \n",
    "#### (1) Get ALL planets that exist in Star Wars, and \n",
    "#### (2) obtain a slice of the above data containing all planets with diameter greater than 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write Python code to achieve the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "After doing all sorts of things to your dataset, you would surely need to save them to disk. Everything you \n",
    "\n",
    "As you'd expect with Python, this is a one liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv file\n",
    "df.to_csv('output.csv')\n",
    "\n",
    "# or if you had formatted it as a json file\n",
    "# df.to_json('output.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
