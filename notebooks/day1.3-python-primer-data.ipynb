{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Primer: working with data\n",
    "\n",
    "## <div style=\"color: #db366d\"> Day 1.3 </div>\n",
    "\n",
    "One of the best things about python is the ability to easily load, explore and manipulate data. As they say, *data is king*, and you can't have good ML outcomes without good data, just like in culinary.\n",
    "\n",
    "![](../images/produce.jpeg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Many famous research papers only focused on dataset building, e.g., \n",
    "\n",
    "- Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6), 141-142.\n",
    "\n",
    "- Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.\n",
    "\n",
    "- Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014, September). Microsoft coco: Common objects in context. In European conference on computer vision (pp. 740-755). Springer, Cham.\n",
    "\n",
    "- Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009, June). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248-255). IEEE. \n",
    "\n",
    "- Tan, C. T., Sapkota, H., & Rosser, D. (2014, April). BeFaced: a casual game to crowdsource facial expressions in the wild. In CHI'14 Extended Abstracts on Human Factors in Computing Systems (pp. 491-494). ACM.\n",
    "\n",
    "*( ok perhaps the last paper is not so famous :) )*\n",
    "\n",
    "<img src=\"../images/befaced.jpg\" width=\"350px\" style=\"padding: 10px 0 0 0\" />\n",
    "<center><a href=\"http://ch3k.com/5\">http://ch3k.com/5</a></center>\n",
    "\n",
    "#### DISCUSSION: Suppose a super-star researcher discovers a novel search algorithm that is proven to produce better targetted search results than Google or Baidu (assuming he has access their algorithms)\n",
    "#### Do you think he should spin off a startup to focus on web search? Why or why not?\n",
    "\n",
    "## Loading data\n",
    "Let's start by reading a text-based file without any fancy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a file from the system into the mem space of this program\n",
    "file = open(\"../data/gowork.csv\", \"r\")\n",
    "\n",
    "# extract the file contents into a variable for us to work with\n",
    "file_contents = file.read()\n",
    "\n",
    "# display the contents on the screen\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's really hard to work with a chunk of text like that...\n",
    "\n",
    "Enter the `pandas` library, probably the most important Python library for data science, e.g., see the pandas' prowess below...\n",
    "\n",
    "![](../images/rise-in-pandas.png)\n",
    "<br/>\n",
    "Let's try loading the same csv file with pandas below:\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import both numpy and pandas, which are often used together\n",
    "# - pandas is built on top of NumPy, a lower level library\n",
    "# - NumPy helps you work with large n-d arrays/matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing common dataset formats is ultra simple in Python\n",
    "df = pd.read_csv('../data/gowork.csv')\n",
    "\n",
    "# the first thing after you load data is always to peek at it\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can also display using the Jupyter Notebook's theme\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic things you can do is to convert a dictionary to a dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# recreating some of the previous data by hand\n",
    "handmade_dict = {\n",
    "    'Breakdown' : ['Slow', 'Yes', 'Slow'],\n",
    "    'Rain'      : [True, True, False],\n",
    "    'Oversleep' : ['Slight', 'Major', 'Major'],\n",
    "    'ML Lecture': [True, True, True]\n",
    "}\n",
    "\n",
    "# note we do not have a variable to hold the df below\n",
    "pd.DataFrame(handmade_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load a lot of common data types in here with Python and pandas.\n",
    "\n",
    "Try the following to load a json file downloaded from data.gov.sg ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# python has a built-in json library to process json\n",
    "# additionally, you need some pandas json libraries \n",
    "# to load it as a pandas DataFrame\n",
    "import json\n",
    "from pandas.io.json import json_normalize # load a specific function\n",
    "\n",
    "# load the file into data var\n",
    "# it also makes sense to look at the file first\n",
    "file = open('../data/datagovsg/weather.json')\n",
    "data =json.load(file)\n",
    "\n",
    "# convert raw json data into a DataFrame\n",
    "df = json_normalize(data['items'],\n",
    "                    record_path=['forecasts'],\n",
    "                    meta=['update_timestamp',\n",
    "                          ['valid_period','start'],\n",
    "                          ['valid_period','end']\n",
    "                         ]\n",
    "                   )\n",
    "\n",
    "# peek at the top 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or peek only bottom 5 rows\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from the web\n",
    "With pandas, you can load data from the web easily as well.\n",
    "\n",
    "Try the following Star Wars web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the requests library to make *web requests*\n",
    "# we will import json and pandas as usual\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# make a web request with the web API\n",
    "# the response from the API will be store in the response var\n",
    "response = requests.get('https://swapi.co/api/people')\n",
    "\n",
    "# get the json formatted text from the response\n",
    "# store it as a dictionary (dict) data structure\n",
    "# dicts are common programming data structures for key-value pairs \n",
    "data_dict = response.json()\n",
    "\n",
    "# let's peek at the dictionary first, to see what we want\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now convert the required (sub)dict into a pandas DataFrame\n",
    "df = pd.DataFrame(data_dict['results'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The universe of Python libraries is vast, and the general advice is to Google for an existing _approach to get your dataset_ (or anything you want to do in Python for that matter) first, before rolling your own.\n",
    "\n",
    "For example, try the follow ultra simple way to get stocks data from the web using pandas_datareader lib.\n",
    "(Note you may need to install it first by issuing `conda install pandas-datareader` command in the terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_datareader consolidates various popular online financial data providers\n",
    "# most are free, or have have free tiers \n",
    "# some don't even require membership, like Stooq, demo'd here\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# fetch Google's stock OHLC data from Stooq and display it\n",
    "df_prices = web.DataReader('GOOG', 'stooq')\n",
    "df_prices.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: \n",
    "#### (1) Fetch current EURUSD, EURGBP, AUDUSD, USDJPY, USDCAD and NZDUSD prices from freeforexapi.com, and \n",
    "#### (2) display in a DataFrame (each currency pair should be a row of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all the libs you'll need\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: write Python code to fetch the required response from freeforexapi.com\n",
    "# hint: read the document on the website and use requests.get(___)\n",
    "response = requests.get(\"https://www.freeforexapi.com/api/live?pairs=EURUSD,USDJPY\")\n",
    "\n",
    "# TODO: get the dict from the response and look at it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a DataFrame from the correct key in the dict\n",
    "# hint: use pd.DataFrame.from_dict(___)\n",
    "\n",
    "# TODO: display the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing the data\n",
    "We saw a bit about how to display the loaded data. How about other information that you'd commonly need?\n",
    "\n",
    "Try the following to obtain basic dimensionality information about datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# basic sizes of the dataframe\n",
    "print('SHAPE of df\\n---------------------')\n",
    "print(df.shape)\n",
    "\n",
    "# detailed info about the dataframe\n",
    "print('\\nDETAILED INFO of df\\n---------------------')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get statistical descriptions of the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data\n",
    "Now that we have the data, we will normally want to do stuff to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g., we can extract a single column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Close'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then work with these columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sum of volume is', df['Volume'].sum())\n",
    "print('Mean of open price is', df['Open'].mean())\n",
    "print('Median of all high prices is', df['High'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing is a key concept when working with DataFrames. It basically means \"cutting\" out a piece from the entire dataset.\n",
    "\n",
    "E.g., we can get a slice at row number (i.e., index=) 100..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice that contains the date \"2018-12-10\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2018-12-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing the 5th to 10th rows...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing rows between Feb and Mar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# note that the rows are sorted in reverse chronological order\n",
    "df.loc['2019-01-31':'2019-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice containing rows from Oct onwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc['2018-10-01':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or slice out records that match certain conditions, e.g., a range of values or equal a certain number...\n",
    "\n",
    "E.g., here's an alternative way to get the previous slice with date \"2018-12-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the way we specify the 'index' col\n",
    "df[df.index == '2018-12-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### EXERCISE: Get a slice of the df_prices data that has *Close* price between 800 and 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: obtain the slice as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data cleansing\n",
    "\n",
    "Some other common things we may want to do with the data includes re-structuring and removing stuff.\n",
    "\n",
    "For this section let's load in the Star Wars dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get('https://swapi.co/api/people')\n",
    "data_dict = response.json()\n",
    "df = pd.DataFrame(data_dict['results'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of ML is knowledge representation, and part of that involves determining what to throw away.\n",
    "\n",
    "After scrutinizing the dataset structure above, the first thing we may want to do is to remove unwanted cols..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that the arg axis=1 tells it to drop the columns (axis=0 refers to rows)\n",
    "df_new = df.drop(['created','films','homeworld','species','starships','url','vehicles'], axis=1) \n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many DataFrame manipulation operations return a new copy of the DataFrame. As shown above, the original DataFrame is untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm dropped cols still here!\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, unless you specify it to be modified **in place**, like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['created','films','homeworld','species','starships','url','vehicles'], axis=1, inplace=True) \n",
    "df # phew..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may want to **rename** some cols to our liking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['birth_yr', 'updated', 'eye_col', 'sex', 'hair_col', 'height', 'weight', 'name', 'skin_col']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're at it, might as well **re-arrange** the cols..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that we're using self-assignment here\n",
    "df = df[['name', 'height', 'weight', 'sex', 'birth_yr', 'eye_col', 'hair_col', 'skin_col', 'updated']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In observing the data, it seems that there are some empty fields (e.g., hair_col), which is common especially in very large datasets.\n",
    "\n",
    "In consequence, we may want to **replace** the non-standard empty fields ('n/a') into something more standard in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN or 'Not a Number' is a standard way to represent nulls in pandas and numpy\n",
    "import numpy as np\n",
    "df.replace(['n/a', 'unknown'], np.nan, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With standard null representations, we can easily do things like viewing **where the nulls are**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or checking **how many nulls in each col**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then after evaluating the nulls, perhaps we may need to **remove the records with nulls**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, the original df is unmodified\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to **drop the columns with nulls** instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is to remove duplicates. We don't have dupes in this data but we can artificially create some like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duped_df = df.append(df)\n",
    "duped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply call the following to **eliminate all dupes**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duped_df.drop_duplicates(inplace=True)\n",
    "duped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of data cleansing is to run some arbitrary function that formats the data to a style that is easier (and often faster) for the ML algorithm to act on.\n",
    "\n",
    "E.g., perhaps we want to **run a function on each row** that converts all dates to our preferred simplified format.\n",
    "\n",
    "First we define the function to run on each date. This function will omit the time portion of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "    return date[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply apply this function on every date in each row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['updated'] = df[\"updated\"].apply(format_date)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL EXERCISE: Can you achieve the above in a single line?\n",
    "#### E.g., now let's **run a function on each row** that rounds off the 'height' to nearest 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code here as usual\n",
    "# hint: lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "After doing all sorts of things to your dataset, you would surely need to save them to disk. Everything you \n",
    "\n",
    "As you'd expect with Python, this is a one liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv file\n",
    "df.to_csv('output.csv')\n",
    "\n",
    "# or if you had formatted it as a json file\n",
    "# df.to_json('output.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: \n",
    "#### (1) Get ALL planets that exist in Star Wars, and \n",
    "#### (2) obtain a slice of the above data containing all planets with diameter greater than 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these'll be all the libs you need\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# init an empty DataFrame to store the result\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# TODO: store the url of the web API\n",
    "# hint: read the API documentation to get the correct URL for planets\n",
    "\n",
    "# TODO: create a loop to keep getting data from the next URL \n",
    "# until you reach a response json received with the \n",
    "# url value corresponding to the 'next' key equals to None\n",
    "# hint: use df.append\n",
    "\n",
    "# TODO: display the resultant DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: additionally, make the index a running sequence again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get a slice of the planets dataset with diameter > 7000\n",
    "# hint: you may need to clean up the data a little first..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
